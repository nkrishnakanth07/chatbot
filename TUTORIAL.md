# ðŸ“š Complete Tutorial: Building a Multi-Document RAG Chatbot

## What You'll Learn

By the end of this tutorial, you'll understand:
- How RAG (Retrieval Augmented Generation) works
- How to process and embed documents
- How to build a semantic search system
- How to create a production-ready API with FastAPI
- How to build a modern React frontend
- How to deploy your application for free

## Prerequisites

**Required Knowledge:**
- Basic Python (functions, classes, async/await)
- Basic JavaScript/React (components, state, props)
- Basic HTTP/REST concepts
- Command line basics

**Required Software:**
- Python 3.9+
- Node.js 16+
- A code editor (VS Code recommended)
- Git

## Part 1: Understanding RAG

### What is RAG?

**RAG = Retrieval Augmented Generation**

Traditional LLMs have a knowledge cutoff and can't access your private documents. RAG solves this by:

1. **Retrieve**: Find relevant information from your documents
2. **Augment**: Add that information to the prompt
3. **Generate**: LLM creates an answer using both its knowledge and your documents

### How Our System Works

```
1. User uploads PDF â†’ Extract text â†’ Split into chunks
                                        â†“
2. Chunks â†’ Create embeddings â†’ Store in vector database
                                        â†“
3. User asks question â†’ Find similar chunks â†’ Add to prompt
                                        â†“
4. LLM generates answer with context from your documents
```

### Why This Architecture?

- **Embeddings**: Convert text to numbers that capture meaning
- **Vector Search**: Find semantically similar text (not just keyword matching)
- **Chunking**: LLMs have token limits, so we split documents into pieces
- **Context Window**: Only send relevant chunks to save tokens and improve accuracy

## Part 2: Backend Deep Dive

### Step 1: Document Processing

**Code Location:** `backend/main.py` - `upload_document()` function

```python
# 1. Read PDF
reader = PdfReader(tmp_path)
text = ""
for page in reader.pages:
    text += page.extract_text()
```

**What's Happening:**
- PyPDF extracts text from each page
- We concatenate all pages into one string

**Why Not OCR?**
- Most PDFs have searchable text already
- OCR is slower and more expensive
- For scanned PDFs, you'd add pytesseract here

### Step 2: Text Chunking

```python
splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
chunks = splitter.split_text(text)
```

**Why 1000 characters?**
- ~250 tokens (rough estimate)
- Small enough for focused context
- Large enough to contain complete thoughts

**Why 200 character overlap?**
- Prevents splitting sentences awkwardly
- Maintains context across chunks
- Helps with questions spanning boundaries

**Try This:**
Experiment with different sizes:
- Smaller (500): More precise but may miss context
- Larger (2000): More context but less focused
- Overlap: Try 0, 100, 300 to see the difference

### Step 3: Creating Embeddings

```python
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
vectorstore = Chroma.from_texts(
    texts=chunks,
    embedding=embeddings,
    metadatas=metadatas,
    persist_directory=f"./chroma_db/{session_id}"
)
```

**What's an Embedding?**
- Text â†’ 1536-dimensional vector
- Similar meaning â†’ similar vectors
- Example: "cat" and "kitten" are close in vector space

**Why ChromaDB?**
- Simple setup (no server required)
- Good for small-medium datasets
- Easy to persist to disk

**Alternatives:**
- Pinecone: Cloud-hosted, better scale
- Weaviate: Self-hosted, more features
- FAISS: Fast, but no persistence by default

### Step 4: The RAG Chain

```python
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=session["vectorstore"].as_retriever(
        search_kwargs={"k": 4}
    ),
    return_source_documents=True
)
```

**What's Happening:**

1. **Retriever**: Searches for k=4 most similar chunks
2. **Chain**: Combines retrieval + LLM generation
3. **Conversational**: Maintains chat history
4. **Source Documents**: Returns which chunks were used

**Why k=4?**
- More chunks = more context but more tokens
- Fewer chunks = less context but faster/cheaper
- 4 is a good balance for most use cases

**Try This:**
Change k to 2, 6, or 10 and see how answers change

### Step 5: Session Management

```python
sessions = {
    "session_id": {
        "vectorstore": ChromaDB,
        "qa_chain": ConversationalRetrievalChain,
        "chat_history": [],
        "documents": []
    }
}
```

**Why Sessions?**
- Isolate different users/conversations
- Each session has its own vectorstore
- Prevents data leakage between users

**Production Improvement:**
Replace in-memory dict with:
```python
# PostgreSQL for metadata
# Redis for session caching
# S3 for document storage
```

### Step 6: Conversation History

```python
chat_history = []
for msg in session["chat_history"]:
    if msg["role"] == "user":
        chat_history.append((msg["content"], ""))
    elif msg["role"] == "assistant":
        chat_history[-1] = (chat_history[-1][0], msg["content"])
```

**Why This Format?**
- LangChain expects tuples: (user_message, assistant_response)
- We convert our array format to LangChain's format
- This maintains context for follow-up questions

**Example:**
```
User: "What is photosynthesis?"
Assistant: "Process where plants convert light to energy..."

User: "How does it work?" â† "it" refers to photosynthesis
```

Without history, the LLM wouldn't know what "it" means.

## Part 3: Frontend Deep Dive

### Step 1: React State Management

```javascript
const [sessionId, setSessionId] = useState(null);
const [messages, setMessages] = useState([]);
const [documents, setDocuments] = useState([]);
```

**State Flow:**
1. Component mounts â†’ create session
2. User uploads â†’ update documents array
3. User sends message â†’ add to messages array
4. Backend responds â†’ add assistant message

### Step 2: API Communication

```javascript
const res = await axios.post(`${API_URL}/upload/${sessionId}`, formData);
```

**Why Axios?**
- Cleaner syntax than fetch()
- Automatic JSON parsing
- Better error handling
- Request/response interceptors

**Alternative:**
```javascript
// Using fetch()
const res = await fetch(`${API_URL}/upload/${sessionId}`, {
  method: 'POST',
  body: formData
});
const data = await res.json();
```

### Step 3: Real-time UI Updates

```javascript
useEffect(() => {
  messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });
}, [messages]);
```

**What This Does:**
- Watches the messages array
- When it changes, scroll to bottom
- Creates smooth chat experience

**Try This:**
Remove this useEffect and see the difference

### Step 4: Loading States

```javascript
{loading && (
  <div className="typing-indicator">
    <span></span><span></span><span></span>
  </div>
)}
```

**UX Principles:**
- Always show loading state
- User knows something is happening
- Reduces perceived wait time

**CSS Animation:**
```css
@keyframes typing {
  0%, 60%, 100% { transform: translateY(0); }
  30% { transform: translateY(-10px); }
}
```

## Part 4: Advanced Topics

### Optimizing Chunk Retrieval

**Current:** Semantic search only
**Better:** Hybrid search

```python
# Add BM25 (keyword search) + semantic search
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

bm25_retriever = BM25Retriever.from_texts(chunks)
ensemble = EnsembleRetriever(
    retrievers=[bm25_retriever, vectorstore.as_retriever()],
    weights=[0.3, 0.7]  # 30% keyword, 70% semantic
)
```

### Adding Streaming Responses

**Current:** Wait for complete response
**Better:** Stream tokens as they generate

```python
# Backend
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = ChatOpenAI(
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# Frontend
const response = await fetch(`${API_URL}/chat/stream/${sessionId}`, {
  method: 'POST',
  body: JSON.stringify({question: input})
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const {value, done} = await reader.read();
  if (done) break;
  const chunk = decoder.decode(value);
  // Update UI with partial response
}
```

### Adding Authentication

```python
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    token = credentials.credentials
    # Verify JWT token here
    if not valid:
        raise HTTPException(401, "Invalid token")
    return user_id

@app.post("/chat/{session_id}")
async def chat(
    session_id: str,
    request: ChatRequest,
    user_id: str = Depends(verify_token)
):
    # Now you have authenticated user_id
    pass
```

### Improving Answer Quality

**Technique 1: Better Prompts**
```python
system_prompt = """You are a helpful assistant. When answering:
1. Use ONLY information from the provided context
2. If unsure, say "I don't have enough information"
3. Cite specific parts of documents
4. Be concise but complete
"""
```

**Technique 2: Re-ranking**
```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

**Technique 3: Query Expansion**
```python
# Generate multiple variations of the question
variations = [
    original_question,
    llm.predict(f"Rephrase this question: {original_question}"),
    llm.predict(f"What's another way to ask: {original_question}")
]
# Retrieve for all variations
```

## Part 5: Debugging & Testing

### Backend Debugging

**1. Check Document Processing**
```python
# Add after text extraction
print(f"Extracted {len(text)} characters")
print(f"First 200 chars: {text[:200]}")
```

**2. Check Embeddings**
```python
# Add after creating vectorstore
print(f"Stored {vectorstore._collection.count()} chunks")
```

**3. Check Retrieval**
```python
# Add in chat endpoint
docs = vectorstore.similarity_search(question, k=4)
print(f"Retrieved {len(docs)} documents")
for doc in docs:
    print(f"- {doc.metadata['filename']}: {doc.page_content[:100]}")
```

### Frontend Debugging

**1. Check API Calls**
```javascript
// Add in sendMessage function
console.log('Sending:', { session_id: sessionId, question: input });
console.log('Response:', res.data);
```

**2. Check State Updates**
```javascript
useEffect(() => {
  console.log('Messages updated:', messages);
}, [messages]);
```

**3. Check Network Tab**
- Open DevTools (F12)
- Go to Network tab
- Watch requests as you interact
- Check request/response payloads

### Common Issues

**Issue:** "Embeddings failed"
**Debug:**
```python
try:
    embeddings = OpenAIEmbeddings()
    test = embeddings.embed_query("test")
    print(f"Embedding dimension: {len(test)}")
except Exception as e:
    print(f"Embedding error: {e}")
```

**Issue:** "No relevant documents found"
**Debug:**
```python
# Lower similarity threshold
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={"score_threshold": 0.5}  # Lower = more results
)
```

## Part 6: Performance Optimization

### Backend Performance

**1. Connection Pooling**
```python
from httpx import AsyncClient

client = AsyncClient()  # Reuse across requests

@app.on_event("startup")
async def startup():
    global client
    client = AsyncClient()

@app.on_event("shutdown")
async def shutdown():
    await client.aclose()
```

**2. Caching**
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_embedding(text: str):
    return embeddings.embed_query(text)
```

**3. Async Processing**
```python
import asyncio

async def process_multiple_docs(files):
    tasks = [process_document(file) for file in files]
    return await asyncio.gather(*tasks)
```

### Frontend Performance

**1. Memoization**
```javascript
import { useMemo, useCallback } from 'react';

const memoizedMessages = useMemo(() => {
  return messages.map(formatMessage);
}, [messages]);

const sendMessage = useCallback(async () => {
  // Function logic
}, [input, sessionId]);
```

**2. Lazy Loading**
```javascript
const [displayedMessages, setDisplayedMessages] = useState([]);

useEffect(() => {
  // Load messages in batches
  const batch = messages.slice(0, displayedMessages.length + 20);
  setDisplayedMessages(batch);
}, [messages]);
```

## Part 7: Going to Production

### Security Checklist

- [ ] Environment variables for all secrets
- [ ] Input validation and sanitization
- [ ] Rate limiting on endpoints
- [ ] HTTPS only (no HTTP)
- [ ] CORS restricted to your domain
- [ ] File upload size limits
- [ ] Authentication and authorization
- [ ] Logging but not sensitive data
- [ ] Regular dependency updates
- [ ] Error messages don't leak info

### Monitoring Setup

**1. Logging**
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
```

**2. Metrics**
```python
from prometheus_client import Counter, Histogram

request_count = Counter('requests_total', 'Total requests')
request_duration = Histogram('request_duration_seconds', 'Request duration')

@app.middleware("http")
async def add_metrics(request, call_next):
    request_count.inc()
    with request_duration.time():
        response = await call_next(request)
    return response
```

### Cost Optimization

**1. Use Cheaper Model for Embeddings**
```python
# Instead of text-embedding-ada-002
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)
# Free and runs locally!
```

**2. Cache Expensive Operations**
```python
# Cache embeddings in database
if embedding_exists_in_db(text):
    return get_embedding_from_db(text)
else:
    embedding = create_embedding(text)
    save_embedding_to_db(text, embedding)
    return embedding
```

**3. Batch Requests**
```python
# Instead of one-by-one
embeddings = openai.Embedding.create(
    input=[chunk1, chunk2, chunk3, ...],  # Batch
    model="text-embedding-ada-002"
)
```

## Conclusion

You now have:
- âœ… A working multi-document RAG chatbot
- âœ… Understanding of how RAG works
- âœ… Knowledge of FastAPI and React
- âœ… Experience with LangChain and OpenAI
- âœ… A portfolio project to show employers

### Next Steps

1. **Enhance It:**
   - Add more file types
   - Implement authentication
   - Add export features
   - Build mobile app

2. **Learn More:**
   - Study advanced RAG techniques
   - Explore fine-tuning LLMs
   - Learn about vector databases
   - Understand prompt engineering

3. **Share It:**
   - Deploy to production
   - Write a blog post
   - Present at meetups
   - Add to portfolio

### Resources

- [LangChain Docs](https://python.langchain.com)
- [OpenAI Cookbook](https://github.com/openai/openai-cookbook)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)
- [React Docs](https://react.dev)

---

**Congratulations!** You've built a production-ready RAG application! ðŸŽ‰
